---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: "{{cookiecutter.projectName}}-service"
  annotations:
    openshift.io/display-name: AI sample application
    description: AI sample application
    tags: ai,service-delivery,{{cookiecutter.projectName}}
labels:
  template: "{{cookiecutter.projectName}}""
parameters:
  # Resource Configuration
  - name: MEMORY_REQUEST
    description: Memory request for the API pods.
    value: "{{cookiecutter.memoryRequest}}"
  - name: MEMORY_LIMIT
    description: Memory limit for the API pods.
    value: "{{cookiecutter.memoryLimit}}"
  - name: CPU_REQUEST
    description: CPU request for the API pods.
    value: "{{cookiecutter.cpuRequest}}"
  - name: CPU_LIMIT
    description: CPU limit for the API pods.
    value: "{{cookiecutter.cpuLimit}}"
  # LLM Client Configuration
  - name: LLM_CLIENT_TYPE
    description: Type of LLM client to use (openai, langchain, llama_stack).
    value: "{{cookiecutter.llmClientType}}"
  - name: LANGCHAIN_PROVIDER
    description: LangChain provider to use (openai, ollama).
    value: "{{cookiecutter.langchainProvider}}"
  # Inference Configuration
  - name: INFERENCE_MODEL_NAME
    description: Model name to use for inference.
    value: "{{cookiecutter.inferenceModelName}}"
  - name: INFERENCE_BASE_URL
    description: Base URL for the inference API endpoint.
    value: "{{cookiecutter.inferenceBaseUrl}}"
  - name: INFERENCE_TEMPERATURE
    description: Temperature for response randomness (0.0-1.0).
    value: "{{cookiecutter.inferenceTemperature}}"
  - name: INFERENCE_MAX_TOKENS
    description: Maximum number of tokens to generate in response.
    value: "{{cookiecutter.inferenceMaxTokens}}"
  - name: SUMMARY_PROMPT
    description: System prompt for the AI assistant.
    value: "{{cookiecutter.summaryPrompt}}"

objects:
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: "{{cookiecutter.projectName}}"
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: "{{cookiecutter.projectName}}"
      template:
        metadata:
          labels:
            app: "{{cookiecutter.projectName}}"
        spec:
          containers:
            - name: "{{cookiecutter.projectName}}"
              image: quay.io/jbarea/hcm-ai-sample-app:1.0
              ports:
                - containerPort: 8000
              resources:
                requests:
                  memory: ${MEMORY_REQUEST}
                  cpu: ${CPU_REQUEST}
                limits:
                  memory: ${MEMORY_LIMIT}
                  cpu: ${CPU_LIMIT}
              env:
                # LLM Client Configuration
                - name: LLM_CLIENT_TYPE
                  value: ${LLM_CLIENT_TYPE}
                - name: LANGCHAIN_PROVIDER
                  value: ${LANGCHAIN_PROVIDER}
                
                # Inference Configuration
                - name: INFERENCE_API_KEY
                  valueFrom:
                    secretKeyRef:
                      name: ${parameter.projectName}-secret
                      key: INFERENCE_API_KEY
                - name: INFERENCE_MODEL_NAME
                  value: ${INFERENCE_MODEL_NAME}
                - name: INFERENCE_BASE_URL
                  value: ${INFERENCE_BASE_URL}
                
                # Optional Configuration (with defaults)
                - name: INFERENCE_TEMPERATURE
                  value: ${INFERENCE_TEMPERATURE}
                - name: INFERENCE_MAX_TOKENS
                  value: ${INFERENCE_MAX_TOKENS}
                - name: SUMMARY_PROMPT
                  value: ${SUMMARY_PROMPT}
              
              livenessProbe:
                httpGet:
                  path: /health
                  port: 8000
                initialDelaySeconds: 30
                periodSeconds: 10
                timeoutSeconds: 5
                failureThreshold: 3
              
              readinessProbe:
                httpGet:
                  path: /health
                  port: 8000
                initialDelaySeconds: 5
                periodSeconds: 5
                timeoutSeconds: 3
                failureThreshold: 2
              
              imagePullPolicy: Always
          securityContext:
            runAsNonRoot: true

  - apiVersion: v1
    kind: Service
    metadata:
      name: "{{cookiecutter.projectName}}"
    spec:
      selector:
        app: "{{cookiecutter.projectName}}"
      ports:
        - protocol: TCP
          port: 443
          targetPort: 8000

  - apiVersion: route.openshift.io/v1
    kind: Route
    metadata:
      name: "{{cookiecutter.projectName}}"
      annotations:
        haproxy.router.openshift.io/timeout: "1200s"
    spec:
      port:
        targetPort: 8000
      to:
        kind: Service
        name: "{{cookiecutter.projectName}}"
        weight: 100
      tls:
        termination: edge
      wildcardPolicy: None

  - apiVersion: v1
    kind: Secret
    metadata:
      name: "{{cookiecutter.projectName}}-secret"
    type: Opaque
    data:
      INFERENCE_API_KEY:  "{{cookiecutter.inferenceApiKey}} | base64"
