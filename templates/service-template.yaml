---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: "{{cookiecutter.projectName}}-service"
  annotations:
    openshift.io/display-name: AI sample application
    description: AI sample application
    tags: ai,service-delivery,{{cookiecutter.projectName}}
labels:
  template: "{{cookiecutter.projectName}}"
parameters:
  # Resource Configuration
  - name: MEMORY_REQUEST
    description: Memory request for the API pods.
    value: "{{cookiecutter.memoryRequest}}"
  - name: MEMORY_LIMIT
    description: Memory limit for the API pods.
    value: "{{cookiecutter.memoryLimit}}"
  - name: CPU_REQUEST
    description: CPU request for the API pods.
    value: "{{cookiecutter.cpuRequest}}"
  - name: CPU_LIMIT
    description: CPU limit for the API pods.
    value: "{{cookiecutter.cpuLimit}}"
  # LLM Client Configuration
  - name: LLM_CLIENT_TYPE
    description: Type of LLM client to use (openai, langchain, llama_stack).
    value: "{{cookiecutter.llmClientType}}"
  - name: LANGCHAIN_PROVIDER
    description: LangChain provider to use (openai, ollama).
    value: "{{cookiecutter.langchainProvider}}"
  # Inference Configuration
  - name: INFERENCE_MODEL_NAME
    description: Model name to use for inference.
    value: "{{cookiecutter.inferenceModelName}}"
  - name: INFERENCE_BASE_URL
    description: Base URL for the inference API endpoint.
    value: "{{cookiecutter.inferenceBaseUrl}}"
  - name: INFERENCE_TEMPERATURE
    description: Temperature for response randomness (0.0-1.0).
    value: "{{cookiecutter.inferenceTemperature}}"
  - name: INFERENCE_MAX_TOKENS
    description: Maximum number of tokens to generate in response.
    value: "{{cookiecutter.inferenceMaxTokens}}"
  - name: SUMMARY_PROMPT
    description: System prompt for the AI assistant.
    value: "{{cookiecutter.systemPrompt}}"

objects:
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: "{{cookiecutter.projectName}}"
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: "{{cookiecutter.projectName}}"
      template:
        metadata:
          labels:
            app: "{{cookiecutter.projectName}}"
        spec:
          containers:
            - name: "{{cookiecutter.projectName}}"
              # TODO: Change this image for the app created image
              image: quay.io/jbarea/hcm-ai-sample-app:1.0
              ports:
                - containerPort: 8000
              resources:
                requests:
                  memory: ${MEMORY_REQUEST}
                  cpu: ${CPU_REQUEST}
                limits:
                  memory: ${MEMORY_LIMIT}
                  cpu: ${CPU_LIMIT}
              env:
                # LLM Client Configuration
                - name: LLM_CLIENT_TYPE
                  value: ${LLM_CLIENT_TYPE}
                - name: LANGCHAIN_PROVIDER
                  value: ${LANGCHAIN_PROVIDER}
                
                # Inference Configuration
                - name: INFERENCE_API_KEY
                  valueFrom:
                    secretKeyRef:
                      key: INFERENCE_API_KEY
                      {% if cookiecutter.inferenceServiceMode == 'Custom' %}
                      name: "{{cookiecutter.projectName}}-secret"
                      {% endif %}
                      {% if cookiecutter.inferenceServiceMode == 'HCMAI' %}
                      name: "hcmai-secret"
                      {% endif %}
                - name: INFERENCE_MODEL_NAME
                  value: ${INFERENCE_MODEL_NAME}
                - name: INFERENCE_BASE_URL
                  value: ${INFERENCE_BASE_URL}
                
                # Optional Configuration (with defaults)
                - name: INFERENCE_TEMPERATURE
                  value: ${INFERENCE_TEMPERATURE}
                - name: INFERENCE_MAX_TOKENS
                  value: ${INFERENCE_MAX_TOKENS}
                - name: SUMMARY_PROMPT
                  value: ${SUMMARY_PROMPT}
              
              livenessProbe:
                httpGet:
                  path: /health
                  port: 8000
                initialDelaySeconds: 30
                periodSeconds: 10
                timeoutSeconds: 5
                failureThreshold: 3
              
              readinessProbe:
                httpGet:
                  path: /health
                  port: 8000
                initialDelaySeconds: 5
                periodSeconds: 5
                timeoutSeconds: 3
                failureThreshold: 2
              
              imagePullPolicy: Always
          securityContext:
            runAsNonRoot: true

  - apiVersion: v1
    kind: Service
    metadata:
      name: "{{cookiecutter.projectName}}"
    spec:
      selector:
        app: "{{cookiecutter.projectName}}"
      ports:
        - protocol: TCP
          port: 443
          targetPort: 8000

  - apiVersion: route.openshift.io/v1
    kind: Route
    metadata:
      name: "{{cookiecutter.projectName}}"
      annotations:
        haproxy.router.openshift.io/timeout: "1200s"
    spec:
      port:
        targetPort: 8000
      to:
        kind: Service
        name: "{{cookiecutter.projectName}}"
        weight: 100
      tls:
        termination: edge
      wildcardPolicy: None

{% if cookiecutter.inferenceServiceMode == 'Custom' %}
  - apiVersion: v1
    kind: Secret
    metadata:
      name: "{{cookiecutter.projectName}}-secret"
    type: Opaque
    data:
      INFERENCE_API_KEY:  "{{cookiecutter.inferenceApiKey}}"
{% endif %}

{% if cookiecutter.chatEnabled %}
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: "chatbot-ui"
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: "chatbot-ui"
      template:
        metadata:
          labels:
            app: "chatbot-ui"
        spec:
          containers:
            - name: "chatbot-ui"
              image: quay.io/jbarea/gradio-chat:1.0.3
              ports:
                - containerPort: 7860
              env:
                - name: INFERENCE_BACKEND_HOST
                  value: "http://{{cookiecutter.projectName}}:443"
                - name: APP_NAME
                  value: "{{cookiecutter.projectName}}"
  - apiVersion: v1
    kind: Service
    metadata:
      name: "chatbot-ui"
    spec:
      selector:
        app: "chatbot-ui"
      ports:
        - protocol: TCP
          port: 443
          targetPort: 7860

  - apiVersion: route.openshift.io/v1
    kind: Route
    metadata:
      name: "chatbot-ui"
    spec:
      port:
        targetPort: 7860
      to:
        kind: Service
        name: "chatbot-ui"
        weight: 100
      tls:
        termination: edge
      wildcardPolicy: None
{% endif %}